[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "An exploration of different summarization techniques. Done as a course project for DAT255 Deep Learning."
  },
  {
    "objectID": "posts/kmeans-clustering/k-means-clustering.html",
    "href": "posts/kmeans-clustering/k-means-clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "When you reach a certain length of text to summarize, the other methods become too expensive or are not able to summarize well enough. We need to find a way to extract all the important parts of large texts like books or very big documents, and create a summary from them.\nSome simple ways to avoid using all the chunks for summarization is either randomly selecting chunks, or spacing out which chunks are selected. But what if we miss out on an important part of the text while doing this?\nA solution is K-means clustering, where each chunk is embedded, and then clusters are formed based on semantic meaning of those chunks. Then a summary is formed from each cluster, hopefully netting us a more accurate summary of huge texts.\nAnother advantage is the amount of requests sent to the API. While Map Reduction sends many requests, clustering will send only one request, saving a lot of money.\n\nfrom dotenv import load_dotenv\nfrom utils import read_files, split_document_by_tokens\nfrom pathlib import Path\n\nimport os\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\ndocuments = read_files(Path('./content/nord-universitet'))\ndoc = documents[1]\n\n\nfrom langchain.schema import Document\n\ncontent = \"\"\n\nfor doc in documents:\n    content += doc.page_content\n\ndoc = Document(page_content=content)\n\nSet up our LLM\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\n\nWe check how many tokens are in our book. Dracula, the book we will summarize, has 216728 tokens. Yikes!\nUsing map reduction or stuffing on this text would cause a lot of tokens to be sent to the LLM, hurting our wallet.\n\nnum_tokens = llm.get_num_tokens(doc.page_content)\nprint(f'Our text has {num_tokens} tokens')\n\nOur text has 137135 tokens\n\n\nSo, we need to start with splitting our book into chunks. To do this I will split by tokens.\n\nsplit_docs = split_document_by_tokens([doc], chunk_size=2000, overlap=200)\nprint(f'Now our document is split up into {len(split_docs)} documents')\n\nNow our document is split up into 93 documents\n\n\n\nEmbedding\nClustering relies on embeddings to work. Embeddings are vector representations of text, so that LLMs can work with them (LLMs don’t understand human readable text, they understand numbers). Therefore we need to create embeddings from our documents.\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model='text-embedding-3-small')\n\nvectors = embeddings.embed_documents([doc.page_content for doc in split_docs])\n\n\n\nClustering\nThere are many different clustering algorithms to choose from. Let’s try a few of them. We’ll start with KMeans clustering. To begin with, we are setting the number of clusters manually, to 11.\n\nfrom sklearn.cluster import KMeans\n\nnum_clusters = 11\n\nkmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(vectors)\n\n\nkmeans.labels_\n\narray([ 8,  8,  8,  8,  5,  5,  5,  1,  6,  6,  6,  6,  6,  4,  4,  4,  4,\n        4,  4,  4,  4,  4,  4,  4,  4,  4,  2,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  3,  3,  3, 10, 10,  2,  2,  2,  2,  5,  5,  5,  5,  1,  1,\n        1,  1,  1,  3,  3,  7,  7,  7,  7,  7,  7, 10,  0,  3,  3,  3,  3,\n        3,  3,  3,  3, 10, 10, 10,  3,  2,  2,  4,  2,  2,  2,  2,  2,  2,\n        7,  2,  2,  2,  9,  9,  9,  9], dtype=int32)\n\n\nTime to visualize\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_clusters(vectors, algo):\n    tsne = TSNE(n_components=2, random_state=42)\n    vectors = np.array(vectors)\n    reduced_data_tsne = tsne.fit_transform(vectors)\n\n    plt.scatter(reduced_data_tsne[:, 0], reduced_data_tsne[:, 1], c=algo.labels_)\n    plt.xlabel('Dimension 1')\n    plt.ylabel('Dimension 2')\n    plt.title('Cluster visualization')\n    plt.show()\n\n\n\nSilhouette Scoring\nBut how can we know that our selected number of clusters is the correct one? We need ways to find the optimal number of clusters for our chunks. A way to do this for KMeans clustering is what is called Silhouette scoring.\n\nfrom sklearn.metrics import silhouette_score\n\ndef find_optimal_clusters(vectors, max_k):\n    score = -1\n    best_n_clusters = 0\n    for k in range(2, max_k):\n        kmeans = KMeans(n_clusters=k, random_state=1).fit_predict(vectors)\n        new_score = silhouette_score(vectors, kmeans)\n        if new_score &gt; score:\n            best_n_clusters = k\n            score = new_score\n        print(f\"For n_clusters = {k}, silhouette score is {new_score})\")\n    print(f\"Best number of clusters is {best_n_clusters}\")\n    return best_n_clusters\n\n\nnum_clusters = find_optimal_clusters(vectors, max_k=20)\nkmeans = KMeans(n_clusters=num_clusters, random_state=1).fit(vectors)\nplot_clusters(vectors, kmeans)\n\nFor n_clusters = 2, silhouette score is 0.23293694126495937)\nFor n_clusters = 3, silhouette score is 0.17584240508130475)\nFor n_clusters = 4, silhouette score is 0.16690842284141724)\nFor n_clusters = 5, silhouette score is 0.11459597345735821)\nFor n_clusters = 6, silhouette score is 0.09039799639025853)\nFor n_clusters = 7, silhouette score is 0.07587504880295566)\nFor n_clusters = 8, silhouette score is 0.11530396408282916)\nFor n_clusters = 9, silhouette score is 0.11039447594718674)\nFor n_clusters = 10, silhouette score is 0.11148730768732826)\nFor n_clusters = 11, silhouette score is 0.10520080243559334)\nFor n_clusters = 12, silhouette score is 0.1120688657473839)\nFor n_clusters = 13, silhouette score is 0.11513871640402203)\nFor n_clusters = 14, silhouette score is 0.11789389085968689)\nFor n_clusters = 15, silhouette score is 0.11291548961109166)\nFor n_clusters = 16, silhouette score is 0.10637631200110255)\nFor n_clusters = 17, silhouette score is 0.10790680413182988)\nFor n_clusters = 18, silhouette score is 0.11641354044021887)\nFor n_clusters = 19, silhouette score is 0.1047930837569986)\nBest number of clusters is 2\n\n\n\n\n\n\n\n\n\nGet the vectors which are closest to the center of the cluster, then sort them so that they are processed in order. Lastly, we fetch the document from that index.\n\ndef get_key_chunks(vectors, alg, num_clusters, documents):\n    closest_indices = []\n\n    for i in range(num_clusters):\n        distances = np.linalg.norm(vectors - alg.cluster_centers_[i], axis=1)\n\n        closest_index = np.argmin(distances)\n        closest_indices.append(closest_index)\n\n    selected_indices = sorted(closest_indices)\n    selected_docs = [documents[doc] for doc in selected_indices]\n    return selected_docs\n\n\nfrom langchain import PromptTemplate\n\nmap_prompt = \"\"\"\nYou will be given a piece of a larger text. This piece of text will be enclosed in triple backticks (```).\nYour job is to give a summary of this piece of text so that the reader will have a full understanding of what the text is about.\nYour response should be at least three paragraphs and fully encompass what was written in the piece of text.\n\n```{text}```\nFULL SUMMARY:\n\"\"\"\n\nmap_prompt_template = PromptTemplate(template=map_prompt, input_variables=[\"text\"])\n\n\nfrom langchain.chains.summarize import load_summarize_chain\n\nmap_chain = load_summarize_chain(llm=llm, chain_type=\"stuff\", prompt=map_prompt_template)\n\nsummary_list = []\n\nselected_docs = get_key_chunks(vectors, kmeans, num_clusters, split_docs)\n\nfor i, doc in enumerate(selected_docs):\n    chunk_summary = map_chain.run([doc])\n    summary_list.append(chunk_summary)\n\n    print(f'Summary #{i} - Preview: {chunk_summary[:250]} \\n')\n\nSummary #0 - Preview: The given text provides recommendations on choosing a Content Management System (CMS) for Nord University. It highlights four platforms that stand out: Wordpress, Episerver, Umbraco, and Craft. Episerver and Umbraco, being part of the Microsoft famil \n\nSummary #1 - Preview: The text outlines the terms and conditions regarding the pricing, extension, and responsibilities of the service provider (Leverandøren) and the customer (Kunden) in a contractual agreement. It specifies that the customer is not required to pay any f \n\n\n\n\nllm4 = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4-turbo-preview\")\n\n\nfrom langchain.schema import Document\nsummaries = \"\\n\".join(summary_list)\n\nsummaries = Document(page_content=summaries)\n\nprint(f'All your summaries together are {llm.get_num_tokens(summaries.page_content)} tokens')\n\nAll your summaries together are 540 tokens\n\n\n\ncombine_prompt = \"\"\"\nYou will now be given a series of summaries from a larger text. The summaries will be enclosed in triple backticks(```).\nYour goal is to give a summary of what happened in the greater piece of text.\nThe reader should be able to grasp what the full text is about from your summary.\n\n```{text}```\nSUMMARY:\n\"\"\"\n\ncombine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=[\"text\"])\n\n\nreduce_chain = load_summarize_chain(llm=llm4, chain_type=\"stuff\", prompt=combine_prompt_template)\n\noutput = reduce_chain.run([summaries])\n\n\nprint(output)\n\nThe larger text provides a comprehensive guide on selecting a Content Management System (CMS) for Nord University, evaluating platforms like Wordpress, Episerver, Umbraco, and Craft based on various criteria including cost, SEO, user experience, and their current phases of development. It emphasizes the need for the university's website to quickly convert visitors into users by being visually appealing, user-friendly, and effectively targeting various user groups to relevant content and landing pages. The text also covers technical aspects of CMS setup such as template design, validation, SEO, user experience checks, and the importance of a streamlined website architecture through access control, data management, and content hierarchy. Additionally, it discusses SEO strategies, sustainability practices, and performance metrics vital for the website's success.\n\nMoreover, the text outlines contractual terms between the service provider and the customer, detailing conditions around fees, termination rights, temporary extensions, and responsibilities related to service delivery, personnel competence, and subcontractor management. It specifies the customer's rights concerning withholding payments for unmet service standards, particularly in relation to labor laws and working conditions, and outlines the obligations of both parties within the service agreement. Overall, the text serves as a detailed guide for Nord University in revamping its website, focusing on both the strategic selection of a CMS platform and the legal and operational considerations of engaging with a service provider."
  },
  {
    "objectID": "posts/presentation/presentation.html",
    "href": "posts/presentation/presentation.html",
    "title": "Presentation",
    "section": "",
    "text": "This is a post with executable code.\n%pip install langchain langchain-community langchain-openai unstructured openai pypdf -Uq\n\n\n[notice] A new release of pip is available: 23.2.1 -&gt; 24.0\n[notice] To update, run: pip3 install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\nfrom langchain.document_loaders import PyPDFLoader, UnstructuredExcelLoader, UnstructuredWordDocumentLoader\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nPERSONAL_API_KEY = os.getenv(\"MY_OPEN_AI_API_KEY\")\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(openai_api_key=PERSONAL_API_KEY, model=\"gpt-4-0125-preview\")"
  },
  {
    "objectID": "posts/presentation/presentation.html#load-test-documents",
    "href": "posts/presentation/presentation.html#load-test-documents",
    "title": "Presentation",
    "section": "Load test documents",
    "text": "Load test documents\nDocument loaders\n\nloader_word = UnstructuredWordDocumentLoader(\"../../content/DPS Kvalifikasjonsgrunnlag - applikasjonsforvaltning V 2.0.docx\")\nloader_pdf = PyPDFLoader(\"../../content/Del-A-Konkurransegrunnlag-aapen-anbudskonkurranse-FOA-del-III xx.pdf\")\nloader_excel = UnstructuredExcelLoader(\"../../content/test.xlsx\")\n\nLoad data from files\n\ndata_word = loader_word.load()\ndata_pdf = loader_pdf.load()\ndata_excel = loader_excel.load()\n\n\nimport re\n\ndef clean_text(text: str):\n    # Remove excessive newlines and keep only ASCII + æøå characters.\n    text = re.sub(r'\\n{2,}', '\\n', text)\n    text = re.sub(r'[^\\x00-\\x7FæøåÆØÅ]+', '', text)\n    # Remove empty strings\n    text = \"\\n\".join([line for line in text.split('\\n') if line.strip() != ''])\n    return text\n\n\nfrom langchain_core.documents import Document\nimport pypdf\n\ndef process_pdf(data) -&gt; Document:\n    \"\"\"\n    Reads a pdf file from the stream, and returns the read text as a Document\n    \"\"\"\n    reader = pypdf.PdfReader(data)\n    text = ''\n    for page_num in range(len(reader.pages)):\n        text += reader.pages[page_num].extract_text()\n    cleaned_text = clean_text(text)\n    doc = Document(page_content=cleaned_text)\n    return doc\n\n\ndoc = process_pdf(\"../../content/Del-A-Konkurransegrunnlag-aapen-anbudskonkurranse-FOA-del-III xx.pdf\")\n\n\nimport csv\ntest_file_name = '../../content/test_file.csv'\nwith open(test_file_name, 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow(['Column1', 'Column2', 'Column3'])\n    writer.writerow(['Data1', 'Data2', 'Data3'])\n\n\nfrom langchain_community.document_loaders import UnstructuredCSVLoader\nloader_csv = UnstructuredCSVLoader(\"../../content/test_file.csv\")\ndata_csv = loader_csv.load()\n\n\ndata_csv\n\n[Document(page_content='\\n\\n\\nColumn1\\nColumn2\\nColumn3\\n\\n\\nData1\\nData2\\nData3\\n\\n\\n', metadata={'source': '../../content/test_file.csv'})]"
  },
  {
    "objectID": "posts/map-reduction/map-reduction.html",
    "href": "posts/map-reduction/map-reduction.html",
    "title": "Map Reduction",
    "section": "",
    "text": "A good way to work around token limitations. If you are summarizing text that is too long for your chosen model’s context window, Map Reduction can be used.\nMap Reduction works like this: - The text is broken up into manageable pieces (chunks) - A summary is generated for each chunk - A final summary is generated from all the chunk summaries\nThis method is well suited for generating summaries of some types of text, but has some limitations: - The model may over or underemphazise certain aspects of the text - Gets really expensive really fast, as you need many calls to the LLM and lots of input tokens.\n\nfrom dotenv import load_dotenv\nfrom utils import read_files, split_document_by_tokens\nfrom pathlib import Path\n\nimport os\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\ndocuments = read_files(Path('../../content/books'))\n\n\nsummary_map_template = \"\"\"Write a short summary of the following text:\n\n{context}\n\nSUMMARY:\n\"\"\"\n\nsummary_reduce_template = \"\"\"The following text is a set of summaries:\n\n{doc_summaries}\n\nCreate a cohesive summary from the above text.\nSUMMARY:\"\"\"\n\n\nfrom langchain.chains import LLMChain, ReduceDocumentsChain, MapReduceDocumentsChain, StuffDocumentsChain\nfrom langchain.docstore.document import Document\nfrom langchain.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI\n\ndef summarize_document(document: list[Document]):\n    # Chain to generate a summary from each chunk\n    llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4-0125-preview\")\n    map_prompt = PromptTemplate.from_template(summary_map_template)\n    map_chain = LLMChain(prompt=map_prompt, llm=llm)\n\n    # Chain to generate one cohesive summary from the summaries\n    reduce_prompt = PromptTemplate.from_template(summary_reduce_template)\n    reduce_chain = LLMChain(prompt=reduce_prompt, llm=llm)\n    stuff_chain = StuffDocumentsChain(llm_chain=reduce_chain, document_variable_name=\"doc_summaries\")\n    reduce_docs_chain = ReduceDocumentsChain(combine_documents_chain=stuff_chain)\n\n    # The complete map reduction chain\n    map_reduce_chain = MapReduceDocumentsChain(\n        llm_chain=map_chain,\n        document_variable_name=\"content\",\n        reduce_documents_chain=reduce_docs_chain\n    )\n\n    splitdocs = split_document_by_tokens(document, 15000, 200)\n    summary = map_reduce_chain.run(splitdocs)\n    return summary"
  },
  {
    "objectID": "posts/summarization/index.html",
    "href": "posts/summarization/index.html",
    "title": "Summarization techniques using Large Language Models",
    "section": "",
    "text": "This blog post talks about our course project in DAT255 Deep Learning Engineering.\nThe course project is related to our bachelor thesis. As part of our project, we are to create summaries for tender competitions (Anbudskonkurranser). These competitions come with a varying amount of documents, with varying length, file format and quality. So, this course project will be preparatory work for our bachelor project. We wish to explore different methods to summarize text using commercially available LLMs (Large Language Models). Since we do not know the specific documents we are to summarize, no specific cleaning or preprocessing can be done, everything has to be generic."
  },
  {
    "objectID": "posts/summarization/index.html#document-summaries-with-langchain",
    "href": "posts/summarization/index.html#document-summaries-with-langchain",
    "title": "Summarization techniques using Large Language Models",
    "section": "Document Summaries with Langchain",
    "text": "Document Summaries with Langchain\n  In this project we have been exploring different methods to summarize text using commercially available LLMs. To do this, we have used the framework Langchain extensively.\nLangchain is a framework for developing applications powered by language models. You can read more about Langchain here.\nThe summarization problem\n  All LLMs have a token limit due to how they are designed and trained. This token limit gives a restriction on how much input you can give the LLM, and how much output you can expect back.\nHere are some well known LLMs and their token limits:\n\n\n\nModel\nToken Limit\nMax Output Tokens\n\n\n\n\nGPT-4-Turbo-Preview\n128 000\n4096\n\n\nGPT-3.5-Turbo\n16 385\n4096\n\n\nClaude 3 Opus/Sonnet/Haiku\n200 000\n4096\n\n\n\nAs we can see, most modern LLMs have a pretty big context window. But what if you want to summarize a huge document with over 500 pages, or maybe you have several documents to combine and summarize? This can quickly exceed the token limit if you want to do this in a single call to the LLM.\nThis means that summarizing the entire document at once is not always feasible, so we need new strategies for generating good summaries for long texts.\nAnd, even if summarizing the entire document is possible, cost is an important factor. Commercial LLMs charge money per input and output token, and summarizing many huge documents will quickly rack up your bill.\n\nStuffing the documents\nDocument stuffing is a method used for smaller documents. Like the name says, this method “Stuffs” the document or documents into the prompt. In Langchain a Chain called “StuffDocumentsChain” is used.\nA StuffDocumentsChain prompt will typically describe the task, and then insert the document(s). However, as we have discussed earlier, this is not a good approch if you have large documents, because this chain only queries the API with one API call containing the whole document.\n\n\n\nMap Reduction\nMap reduction is another common approach for summarizing documents. This method is able to summarize documents which exceed the LLMs token limit by first breaking the documents into chunks which fit in the context window, then generating summaries for each chunk, and lastly generating a final summary from all the summaries.\nThis method lets us generate summaries for texts of arbitrary length (if the combined summaries are still too long, you can generate a summary from a group of summaries until they will fit in the context window), but as discussed earlier, cost is a problem. Map Reduction does many calls to the API, and will use a lot of input and output tokens in the process."
  },
  {
    "objectID": "posts/summarization/index.html#clustering",
    "href": "posts/summarization/index.html#clustering",
    "title": "Summarization techniques using Large Language Models",
    "section": "Clustering",
    "text": "Clustering\nIn this method, you first break the document into chunks, then generate embeddings from these chunks. An embedding in this context is a vector representation of text. We use embeddings because machine learning models work with numbers only, and cannot understand human readable text directly. Embeddings contain many dimensions and captures the semantic and syntactic meaning of a piece of text. If you embed many different words, semantically similar words like tree and forest will end up closer together in the vector space than semantically different words like lion and truck.\nWe can use this to our advantage when summarizing documents. If we embed all our chunks, then chunks that are talking about the same topic will be closer together in the vector space. Then, we can cluster the chunks together based on their semantic meaning. The image below shows a “squashed down” visualization (the embedding vectors have a dimension of 1536, here they are reduced to 2 dimensions) of the embedding vector space for a tender competition.\n\nTo create these clusters, a clustering algorithm is used. The algorithm identifies clusters, then we find the center of the cluster and extract the nearest chunk, which will represent the “average meaning” of that cluster.\nThe goal of this method is to identify key topics in the text and assemble them to create a context-rich summary while spending as little as possible on API fees.\n\nSources / Further reading\nSummarizing long documents with AI - https://pashpashpash.substack.com/p/tackling-the-challenge-of-document\n5 Levels of Summarization: Novice to Expert - https://github.com/gkamradt/langchain-tutorials/blob/main/data_generation/5%20Levels%20Of%20Summarization%20-%20Novice%20To%20Expert.ipynb\nClustering for QA - https://github.com/mendableai/QA_clustering/blob/main/notebooks/clustering_approach.ipynb"
  },
  {
    "objectID": "posts/kmeans-clustering/clustering.html",
    "href": "posts/kmeans-clustering/clustering.html",
    "title": "KMeans Clustering",
    "section": "",
    "text": "When you reach a certain length of text to summarize, the other methods become too expensive or are not able to summarize well enough. We need to find a way to extract all the important parts of large texts like books or very big documents, and create a summary from them.\nSome simple ways to avoid using all the chunks for summarization is either randomly selecting chunks, or spacing out which chunks are selected. But what if we miss out on an important part of the text while doing this?\nA solution is K-means clustering, where each chunk is embedded, and then clusters are formed based on semantic meaning of those chunks. Then a summary is formed from each cluster, hopefully netting us a more accurate summary of huge texts.\nAnother advantage is the amount of requests sent to the API. While Map Reduction sends many requests, clustering will send only one request, saving a lot of money.\n\nLoading our documents\nLet’s implement this method to summarize many documents or whole books!\nFirst, we need to import some packages and load in our OpenAI API key, since we will be using OpenAI’s GPT models.\n\n\nCode\nfrom dotenv import load_dotenv\nfrom utils import read_files, split_document_by_tokens\nfrom pathlib import Path\nimport os\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n\n\nNext, we will load in our documents. Let’s load in a whole tender competition and a whole book (Pride and Prejudice).\n\nanbudskonkurranse = read_files(Path('./content/nord-universitet'))\nanbudskonkurranse_filnavn = [doc.metadata[\"source\"] for doc in anbudskonkurranse]\nprint(\"Tender competition documents:\\n\")\nfor konk in anbudskonkurranse_filnavn:\n    print(konk)\n\nbooks = read_files(Path(\"./content/books/\"))\npride_and_prejudice = books[0]\nprint(\"\\nBook:\\n\")\nprint(pride_and_prejudice.metadata[\"source\"])\n\nTender competition documents:\n\ncontent/nord-universitet/Vedlegg_4_Profilhandbok-Nord-universitet.pdf\ncontent/nord-universitet/Vedlegg_5.B_databehandleravtale_bilag_2020.docx\ncontent/nord-universitet/Vedlegg_3_CMS-anbefaling-Leveranse-Nord-universitet.pdf\ncontent/nord-universitet/Vedlegg_2_Analyse-og-evaluering-Leveranse-Nord-universitet.pdf\ncontent/nord-universitet/ssa-v_bilag_2018_bok (1).docx\ncontent/nord-universitet/ssa-v_generell_avtaletekst_2018_bok (2).docx\ncontent/nord-universitet/ssa-t-bilag-2.docx\ncontent/nord-universitet/Vedlegg_1_FS-integrasjon.pdf\ncontent/nord-universitet/Vedlegg_5.A_dba_generell_avtaletekst_2020_no.docx\ncontent/nord-universitet/Vedlegg_6_Retningslinjer-for-www-nord-no-vedtatt-februar-2021.pdf\ncontent/nord-universitet/ssa-t_generell-avtaletekst-2018-bok.docx\ncontent/nord-universitet/ssa-v-bilag-2.docx\ncontent/nord-universitet/ssa-t_bilag_2018_bok.docx\ncontent/nord-universitet/Vedlegg_7_Prisskjema.xlsx\ncontent/nord-universitet/KONKURRANSEGRUNNLAG.docx\n\nBook:\n\ncontent/books/pride_and_prejudice.pdf\n\n\nLet’s see how many tokens are in our documents!\n\n\nCode\nfrom langchain_openai import ChatOpenAI\nfrom langchain.schema import Document\n\nllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\n\ntender_content = \"\"\n\nfor doc in anbudskonkurranse:\n    tender_content += doc.page_content\n\ntender_documents = Document(page_content=tender_content)\n\nnum_tokens_tender = llm.get_num_tokens(tender_documents.page_content)\nprint(f\"Number of tokens in tender competition documents: {num_tokens_tender}\")\n\nnum_tokens_book = llm.get_num_tokens(pride_and_prejudice.page_content)\nprint(f\"Number of tokens in book: {num_tokens_book}\")\n\n\nNumber of tokens in tender competition documents: 137133\nNumber of tokens in book: 160831\n\n\nThat’s a lot of tokens. If we were to use map reduction, sending all these tokens to the LLM would be pretty expensive. If we were using document stuffing, we might not be able to fit the document(s) at all!\n\n\nChunking our documents\nTo better facilitate clustering, let’s split our documents into more manageable sizes, called chunks. There are many different ways to chunk a document, the easiest being to split on specific characters, like punctuation marks. Another method is to chunk on token count, this can be nice because you know how many tokens you’re sending to the LLM with each chunk.\nLet’s try with token chunking.\n\nsplit_tender_competition = split_document_by_tokens(anbudskonkurranse, chunk_size=2000, overlap=200)\n\nsplit_book = split_document_by_tokens([pride_and_prejudice], chunk_size=2000, overlap=200)\n\nprint(f\"Now our tender competition is split up into {len(split_tender_competition)} documents\")\n\nprint(f\"And our book is split up into {len(split_book)} documents\")\n\nNow our tender competition is split up into 98 documents\nAnd our book is split up into 92 documents\n\n\n\n\nEmbedding\nClustering relies on embeddings to work. Embeddings are vector representations of text, so that LLMs can work with them (LLMs don’t understand human readable text, they understand numbers). Similar pieces of text will be closer together in the vector space, therefore the hope is that we can “cluster” pieces of text with similar meaning together, because they are closer together in the vector space.\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\")\n\ntender_vectors = embeddings.embed_documents([doc.page_content for doc in split_tender_competition])\n\nbook_vectors = embeddings.embed_documents([doc.page_content for doc in split_book])\n\n\n\nCode\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_clusters(vectors, title, algo = None, ):\n    tsne = TSNE(n_components=2, random_state=42)\n    vectors = np.array(vectors)\n    reduced_data_tsne = tsne.fit_transform(vectors)\n    if algo:\n        plt.scatter(reduced_data_tsne[:, 0], reduced_data_tsne[:, 1], c=algo.labels_)\n    else:\n        plt.scatter(reduced_data_tsne[:, 0], reduced_data_tsne[:, 1])\n    plt.xlabel('Dimension 1')\n    plt.ylabel('Dimension 2')\n    plt.title(f'Cluster visualization for {title}.')\n    plt.show()\n\n\nLet’s try to visualize how our chunks look in the vector space. Please note that the embedding vectors have 1536 dimensions, and here we have squished them down to two dimension, so a lot of information will be lost. Perhaps some clustering of chunks would be easier to see with more dimensions.\n\nplot_clusters(vectors=book_vectors, title=\"Pride and Prejudice\")\nplot_clusters(vectors=tender_vectors, title=\"the tender competition\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering\nK-means clustering is an algorithm which starts by randomly initializing the centroids of a given amount of clusters (k). A centroid is one datapoint in the center of a cluster. It then iterates through two main steps: assignment and update.\nIn the assignment step, each data point is assigned to the closest centroid based on a distance metric, typically Euclidean distance. In the update step, the centroids are recalculated as the mean of all data points assigned to each cluster.\nThis process repeats until the centroids no longer move significantly, indicating convergence. The result is a grouping of data points such that points in the same cluster are more similar to each other than to those in other clusters, based on the chosen distance metric.\nNote that K-means takes the amount of clusters as a parameter. For now, we’ll set it manually.\n\nfrom sklearn.cluster import KMeans\n\nnum_clusters = 11\n\ntender_kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(tender_vectors)\n\nbook_kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(book_vectors)\n\nNow let’s visualize our clusters:\n\n\nCode\nplot_clusters(vectors=tender_vectors, algo=tender_kmeans, title=\"the tender competition with 11 clusters\")\nplot_clusters(vectors=book_vectors, algo=book_kmeans, title=\"Pride and Prejudice with 11 clusters\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis looks alright, especially the tender competition has some clear clusters. However, how can we know that we have chosen the correct amount of clusters for our K-means algorithm?\n\n\nSilhouette scoring\nSilhouette scoring is a method used to assess the quality of clusters when using for example K-means. The silhouette score for each data point is calculated based on two factors: - The average distance between the data point and all other points in the same cluster (cohesion) - The average distance between the data point and all points in the nearest cluster to which the data point does not belong (separation).\nWe find the silhouette score for one data point by taking the difference between these two distances, normalized by the maximum of the two. This score ranges from -1 to 1, where a score close to 1 indicates that the data point clearly belongs to its own cluster, a score near 0 indicates that the point is on the border of two clusters, and a score near -1 suggests that the point may have been assigned to the wrong cluster. The overall silhouette score of the clustering is the average of the silhouette scores of all the data points.\nSo what we can do is try many different values of k and find the silhouette score for each one, then choose the best scoring k as our cluster amount, where the score closest to 1 is best.\n\nfrom sklearn.metrics import silhouette_score\n\ndef find_optimal_clusters(vectors, max_k):\n    score = -1\n    best_n_clusters = 0\n    for k in range(3, max_k):\n        kmeans = KMeans(n_clusters=k, random_state=1).fit_predict(vectors)\n        new_score = silhouette_score(vectors, kmeans)\n        if new_score &gt; score:\n            best_n_clusters = k\n            score = new_score\n        print(f\"For n_clusters = {k}, silhouette score is {new_score})\")\n    print(f\"Best number of clusters is {best_n_clusters}\")\n    return best_n_clusters\n\nLet’s find the optimal number of cluster for our tender competition:\n\ntender_num_clusters = find_optimal_clusters(tender_vectors, max_k=20)\ntender_kmeans = KMeans(n_clusters=tender_num_clusters, random_state=1).fit(tender_vectors)\nplot_clusters(vectors=tender_vectors, algo=tender_kmeans, title=f\"our tender competition with {tender_num_clusters} clusters\")\n\nFor n_clusters = 3, silhouette score is 0.16319734949495834)\nFor n_clusters = 4, silhouette score is 0.13677655311745174)\nFor n_clusters = 5, silhouette score is 0.13867432915900288)\nFor n_clusters = 6, silhouette score is 0.11354961404064279)\nFor n_clusters = 7, silhouette score is 0.10669988485818448)\nFor n_clusters = 8, silhouette score is 0.10039881578450706)\nFor n_clusters = 9, silhouette score is 0.0775684824083643)\nFor n_clusters = 10, silhouette score is 0.07354397576934713)\nFor n_clusters = 11, silhouette score is 0.0896860457734401)\nFor n_clusters = 12, silhouette score is 0.09227585159583783)\nFor n_clusters = 13, silhouette score is 0.09388758282715984)\nFor n_clusters = 14, silhouette score is 0.09267778721302193)\nFor n_clusters = 15, silhouette score is 0.09460706544627888)\nFor n_clusters = 16, silhouette score is 0.0869772390044283)\nFor n_clusters = 17, silhouette score is 0.09299719039273895)\nFor n_clusters = 18, silhouette score is 0.10369596708641589)\nFor n_clusters = 19, silhouette score is 0.10545737216311063)\nBest number of clusters is 3\n\n\n\n\n\n\n\n\n\nAnd then for our book:\n\n\nCode\nbook_num_clusters = find_optimal_clusters(book_vectors, max_k=20)\nbook_kmeans = KMeans(n_clusters=book_num_clusters, random_state=1).fit(book_vectors)\nplot_clusters(vectors=book_vectors, algo=book_kmeans, title=f\"Pride and Prejudice with {book_num_clusters} clusters\")\n\n\nFor n_clusters = 3, silhouette score is 0.047733092427202854)\nFor n_clusters = 4, silhouette score is 0.051067859191365084)\nFor n_clusters = 5, silhouette score is 0.05523242135225988)\nFor n_clusters = 6, silhouette score is 0.0465462151274959)\nFor n_clusters = 7, silhouette score is 0.030455545138270337)\nFor n_clusters = 8, silhouette score is 0.03968968584609667)\nFor n_clusters = 9, silhouette score is 0.03900376290198097)\nFor n_clusters = 10, silhouette score is 0.0393203646925307)\nFor n_clusters = 11, silhouette score is 0.041871536535647465)\nFor n_clusters = 12, silhouette score is 0.04602273803755732)\nFor n_clusters = 13, silhouette score is 0.04664794070136)\nFor n_clusters = 14, silhouette score is 0.04450307354685479)\nFor n_clusters = 15, silhouette score is 0.037468937738576666)\nFor n_clusters = 16, silhouette score is 0.036899065375207624)\nFor n_clusters = 17, silhouette score is 0.04084609100393472)\nFor n_clusters = 18, silhouette score is 0.04017819773375103)\nFor n_clusters = 19, silhouette score is 0.039037963267425264)\nBest number of clusters is 5\n\n\n\n\n\n\n\n\n\n\n\nSummarization using K-means\nNow for the final piece of the puzzle. We have the clusters, now what? Let’s find the chunk which sits closest to the center of each cluster.\n\ndef get_key_chunks(vectors, alg, num_clusters, documents):\n    closest_indices = []\n\n    for i in range(num_clusters):\n        distances = np.linalg.norm(vectors - alg.cluster_centers_[i], axis=1)\n\n        closest_index = np.argmin(distances)\n        closest_indices.append(closest_index)\n\n    selected_indices = sorted(closest_indices)\n    selected_docs = [documents[doc] for doc in selected_indices]\n    return selected_docs\n\nThe centroid chunks will be used as the “average” chunk for each cluster, hopefully giving us a solid idea of what the entire cluster is talking about.\nNow that we have the chunks, let’s do some good old map reduction on them:\n\nfrom langchain import PromptTemplate\nfrom langchain.chains.summarize import load_summarize_chain\n\nllm4 = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4-turbo\")\n\nmap_prompt = \"\"\"\nYou will be given a piece of a larger text. This piece of text will be enclosed in triple backticks (```).\nYour job is to give a summary of this piece of text so that the reader will have a full understanding of what the text is about.\nYour response should be at least three paragraphs and fully encompass what was written in the piece of text.\n\n```{text}```\n\nFULL SUMMARY:\n\"\"\"\n\nmap_prompt_template = PromptTemplate(template=map_prompt, input_variables=[\"text\"])\n\nmap_chain = load_summarize_chain(llm=llm, chain_type=\"stuff\", prompt=map_prompt_template)\n\nWe once again start with the tender competition:\n\nsummary_list = []\n\nselected_tender_docs = get_key_chunks(tender_vectors, tender_kmeans, tender_num_clusters, split_tender_competition)\n\nfor i, doc in enumerate(selected_tender_docs):\n    chunk_summary = map_chain.run([doc])\n    summary_list.append(chunk_summary)\n\n    print(f'Summary #{i} - Preview: {chunk_summary[:250]} \\n')\n\ntender_summaries = \"\\n\".join(summary_list)\n\ntender_summaries = Document(page_content=tender_summaries)\n\nprint(f'All your summaries together are {llm.get_num_tokens(tender_summaries.page_content)} tokens')\n\nSummary #0 - Preview: The text discusses the need for the university's website to be more visually appealing and user-friendly while still maintaining a sense of creativity. It emphasizes the importance of standing out among other universities by daring to have a clear pe \n\nSummary #1 - Preview: The piece of text provided outlines various important clauses and terms within a contractual agreement. It emphasizes the importance of confidentiality, stating that both parties must maintain confidentiality even after the agreement has ended. Emplo \n\nSummary #2 - Preview: The piece of text provided outlines the requirements for a Content Management System (CMS) solution for Nord University. It emphasizes the importance of having a solution that can efficiently manage broken links, redirects, user-friendly interfaces,  \n\nAll your summaries together are 801 tokens\n\n\nAnd then, finally, we combine the summaries.\n\ncombine_prompt = \"\"\"\nYou will now be given a series of summaries from a larger text. The summaries will be enclosed in triple backticks(```).\nYour goal is to give a summary of what happened in the greater piece of text.\nThe reader should be able to grasp what the full text is about from your summary.\n\n```{text}```\nSUMMARY:\n\"\"\"\n\ncombine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=[\"text\"])\n\nreduce_chain = load_summarize_chain(llm=llm4, chain_type=\"stuff\", prompt=combine_prompt_template)\n\n\nimport textwrap\ntender_output = reduce_chain.run([tender_summaries])\ntextwrap.wrap(text=tender_output, width=100, replace_whitespace=False)\n\n[\"The larger text primarily discusses the enhancement and management of Nord University's digital\",\n 'presence, focusing on three key areas: website design and user experience, contractual obligations',\n 'and terms for services, and specifications for a Content Management System (CMS). \\n\\nFirstly, the',\n \"text outlines the importance of redesigning the university's website to be more visually appealing,\",\n \"user-friendly, and accessible, while emphasizing the institution's unique qualities and research\",\n 'activities. It suggests improvements such as simplifying language, using engaging content, and',\n 'ensuring universal design to attract potential students and researchers.\\n\\nSecondly, the text delves',\n 'into detailed contractual terms and conditions between parties, stressing the significance of',\n 'confidentiality, compensation specifics, payment terms, legal compliance, and security measures.',\n 'This section provides a comprehensive overview of the expectations and responsibilities to ensure',\n 'smooth contractual relationships.\\n\\nLastly, the text specifies requirements for a CMS solution',\n 'tailored for Nord University. It emphasizes functionalities like managing broken links, user',\n 'authentication, profile management, workflow processes, and document accessibility to enhance',\n \"content management and user experience on the university's digital platforms.\\n\\nOverall, the text\",\n \"serves as a guide to improving and managing various aspects of Nord University's digital strategy to\",\n 'better serve its community and stakeholders.']\n\n\nLastly, let’s repeat that process for Pride and Prejudice:\n\n\nCode\nsummary_list = []\n\nselected_book_docs = get_key_chunks(book_vectors, book_kmeans, book_num_clusters, split_book)\n\nfor i, doc in enumerate(selected_book_docs):\n    chunk_summary = map_chain.run([doc])\n    summary_list.append(chunk_summary)\n\n    print(f'Summary #{i} - Preview: {chunk_summary[:250]} \\n')\n\nbook_summaries = \"\\n\".join(summary_list)\n\nbook_summaries = Document(page_content=book_summaries)\n\nprint(f'All your summaries together are {llm.get_num_tokens(book_summaries.page_content)} tokens')\n\n\nSummary #0 - Preview: The piece of text revolves around a discussion between Elizabeth and another character (likely Jane) regarding Mr. Bingley's behavior and relationships. Elizabeth expresses her belief that Mr. Bingley's actions are not driven by design but rather by  \n\nSummary #1 - Preview: In this piece of text, the characters Elizabeth, Mrs. Gardiner, and others are discussing Mr. Darcy's behavior and character. Mrs. Gardiner expresses surprise at Darcy's treatment of Mr. Wickham, noting his pleasant appearance but questioning his act \n\nSummary #2 - Preview: The enclosed text depicts a conversation between Elizabeth and her family regarding the elopement of Lydia with Wickham. Elizabeth expresses concerns about Lydia's lack of moral values and upbringing, attributing her actions to her youth, frivolous l \n\nSummary #3 - Preview: In this piece of text, we see Elizabeth Bennet feeling disappointed and hurt by Mr. Darcy's behavior when they meet again after some time. Despite her attempts to engage him in conversation, Mr. Darcy remains silent and distant, causing Elizabeth to  \n\nSummary #4 - Preview: In this piece of text, we see a scene where Jane Bennet's engagement to Mr. Bingley is announced and celebrated by her family. Mrs. Bennet is ecstatic and expresses her happiness at the match, believing Jane to be the luckiest and most beautiful of h \n\nAll your summaries together are 1593 tokens\n\n\n\n\nCode\nbook_output = reduce_chain.run([book_summaries])\ntextwrap.wrap(text=book_output, width=100, replace_whitespace=False)\n\n\n['The overarching narrative encapsulated in the text pieces revolves around the intricate social',\n 'dynamics and emotional developments primarily within the Bennet family from Jane Austen\\'s \"Pride and',\n 'Prejudice.\" The story threads highlight delicate conversations and evolving relationships influenced',\n 'by societal norms, romantic entanglements, and personal values.\\n\\nCentral to the narrative is',\n 'Elizabeth Bennet, whose astute observations and interactions with characters like Mr. Bingley, Mr.',\n 'Darcy, and her own family members drive much of the plot. Discussions often revolve around the',\n \"complexities of relationships, such as Mr. Bingley's fluctuating attentions and Mr. Darcy's\",\n 'seemingly aloof but complex disposition, which Elizabeth tries to decipher. Key events, such as Mr.',\n \"Bingley's behavior towards Jane Bennet, Lydia Bennet's reckless elopement with Mr. Wickham, and the\",\n 'unexpected visit from Lady Catherine de Bourgh, catalyze shifts in these relationships and societal',\n \"perceptions.\\n\\nThe narrative captures the tension and excitement surrounding Jane Bennet's engagement\",\n \"to Mr. Bingley, juxtaposed with Elizabeth's personal turmoil in understanding her feelings towards\",\n 'Mr. Darcy amidst societal and familial expectations. The text also explores themes of morality,',\n 'reputation, and personal growth as characters navigate the challenges posed by their desires,',\n 'obligations, and the prevailing social etiquette.\\n\\nOverall, the text pieces collectively portray the',\n 'emotional landscape and social intricacies of early 19th-century England through the lens of the',\n \"Bennet family's experiences, focusing on romance, social status, and personal integrity.\"]\n\n\nAnd that’s it!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploration of summarization techniques using machine learning",
    "section": "",
    "text": "Clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummarization techniques using Large Language Models\n\n\n\n\n\n\nllm\n\n\n\n\n\n\n\n\n\nApr 16, 2024\n\n\nNora Kristiansen og Torbjørn Vatnelid\n\n\n\n\n\n\n\n\n\n\n\n\nPresentation\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nllm\n\n\n\n\n\n\n\n\n\nApr 15, 2024\n\n\nNora Kristiansen, Torjørn Vatnelid\n\n\n\n\n\n\n\n\n\n\n\n\nKMeans Clustering\n\n\n\n\n\n\nllm\n\n\n\n\n\n\n\n\n\nApr 15, 2024\n\n\nNora Kristiansen og Torbjørn Vatnelid\n\n\n\n\n\n\n\n\n\n\n\n\nMap Reduction\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nllm\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\nNora Kristiansen, Torjørn Vatnelid\n\n\n\n\n\n\nNo matching items"
  }
]