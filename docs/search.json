[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "An exploration of different summarization techniques. Done as a course project for DAT255 Deep Learning."
  },
  {
    "objectID": "posts/kmeans-clustering/k-means-clustering.html",
    "href": "posts/kmeans-clustering/k-means-clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "When you reach a certain length of text to summarize, the other methods become too expensive or are not able to summarize well enough. We need to find a way to extract all the important parts of large texts like books or very big documents, and create a summary from them.\nSome simple ways to avoid using all the chunks for summarization is either randomly selecting chunks, or spacing out which chunks are selected. But what if we miss out on an important part of the text while doing this?\nA solution is K-means clustering, where each chunk is embedded, and then clusters are formed based on semantic meaning of those chunks. Then a summary is formed from each cluster, hopefully netting us a more accurate summary of huge texts.\nAnother advantage is the amount of requests sent to the API. While Map Reduction sends many requests, clustering will send only one request, saving a lot of money.\n\nfrom dotenv import load_dotenv\nfrom utils import read_files, split_document_by_tokens\nfrom pathlib import Path\n\nimport os\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\ndocuments = read_files(Path('./content/nord-universitet'))\ndoc = documents[1]\n\n\nfrom langchain.schema import Document\n\ncontent = \"\"\n\nfor doc in documents:\n    content += doc.page_content\n\ndoc = Document(page_content=content)\n\nSet up our LLM\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\n\nWe check how many tokens are in our book. Dracula, the book we will summarize, has 216728 tokens. Yikes!\nUsing map reduction or stuffing on this text would cause a lot of tokens to be sent to the LLM, hurting our wallet.\n\nnum_tokens = llm.get_num_tokens(doc.page_content)\nprint(f'Our text has {num_tokens} tokens')\n\nOur text has 137135 tokens\n\n\nSo, we need to start with splitting our book into chunks. To do this I will split by tokens.\n\nsplit_docs = split_document_by_tokens([doc], chunk_size=2000, overlap=200)\nprint(f'Now our document is split up into {len(split_docs)} documents')\n\nNow our document is split up into 93 documents\n\n\n\nEmbedding\nClustering relies on embeddings to work. Embeddings are vector representations of text, so that LLMs can work with them (LLMs don’t understand human readable text, they understand numbers). Therefore we need to create embeddings from our documents.\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model='text-embedding-3-small')\n\nvectors = embeddings.embed_documents([doc.page_content for doc in split_docs])\n\n\n\nClustering\nThere are many different clustering algorithms to choose from. Let’s try a few of them. We’ll start with KMeans clustering. To begin with, we are setting the number of clusters manually, to 11.\n\nfrom sklearn.cluster import KMeans\n\nnum_clusters = 11\n\nkmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(vectors)\n\n\nkmeans.labels_\n\narray([ 8,  8,  8,  8,  5,  5,  5,  1,  6,  6,  6,  6,  6,  4,  4,  4,  4,\n        4,  4,  4,  4,  4,  4,  4,  4,  4,  2,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  3,  3,  3, 10, 10,  2,  2,  2,  2,  5,  5,  5,  5,  1,  1,\n        1,  1,  1,  3,  3,  7,  7,  7,  7,  7,  7, 10,  0,  3,  3,  3,  3,\n        3,  3,  3,  3, 10, 10, 10,  3,  2,  2,  4,  2,  2,  2,  2,  2,  2,\n        7,  2,  2,  2,  9,  9,  9,  9], dtype=int32)\n\n\nTime to visualize\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_clusters(vectors, algo):\n    tsne = TSNE(n_components=2, random_state=42)\n    vectors = np.array(vectors)\n    reduced_data_tsne = tsne.fit_transform(vectors)\n\n    plt.scatter(reduced_data_tsne[:, 0], reduced_data_tsne[:, 1], c=algo.labels_)\n    plt.xlabel('Dimension 1')\n    plt.ylabel('Dimension 2')\n    plt.title('Cluster visualization')\n    plt.show()\n\n\n\nSilhouette Scoring\nBut how can we know that our selected number of clusters is the correct one? We need ways to find the optimal number of clusters for our chunks. A way to do this for KMeans clustering is what is called Silhouette scoring.\n\nfrom sklearn.metrics import silhouette_score\n\ndef find_optimal_clusters(vectors, max_k):\n    score = -1\n    best_n_clusters = 0\n    for k in range(2, max_k):\n        kmeans = KMeans(n_clusters=k, random_state=1).fit_predict(vectors)\n        new_score = silhouette_score(vectors, kmeans)\n        if new_score &gt; score:\n            best_n_clusters = k\n            score = new_score\n        print(f\"For n_clusters = {k}, silhouette score is {new_score})\")\n    print(f\"Best number of clusters is {best_n_clusters}\")\n    return best_n_clusters\n\n\nnum_clusters = find_optimal_clusters(vectors, max_k=20)\nkmeans = KMeans(n_clusters=num_clusters, random_state=1).fit(vectors)\nplot_clusters(vectors, kmeans)\n\nFor n_clusters = 2, silhouette score is 0.23293694126495937)\nFor n_clusters = 3, silhouette score is 0.17584240508130475)\nFor n_clusters = 4, silhouette score is 0.16690842284141724)\nFor n_clusters = 5, silhouette score is 0.11459597345735821)\nFor n_clusters = 6, silhouette score is 0.09039799639025853)\nFor n_clusters = 7, silhouette score is 0.07587504880295566)\nFor n_clusters = 8, silhouette score is 0.11530396408282916)\nFor n_clusters = 9, silhouette score is 0.11039447594718674)\nFor n_clusters = 10, silhouette score is 0.11148730768732826)\nFor n_clusters = 11, silhouette score is 0.10520080243559334)\nFor n_clusters = 12, silhouette score is 0.1120688657473839)\nFor n_clusters = 13, silhouette score is 0.11513871640402203)\nFor n_clusters = 14, silhouette score is 0.11789389085968689)\nFor n_clusters = 15, silhouette score is 0.11291548961109166)\nFor n_clusters = 16, silhouette score is 0.10637631200110255)\nFor n_clusters = 17, silhouette score is 0.10790680413182988)\nFor n_clusters = 18, silhouette score is 0.11641354044021887)\nFor n_clusters = 19, silhouette score is 0.1047930837569986)\nBest number of clusters is 2\n\n\n\n\n\n\n\n\n\nGet the vectors which are closest to the center of the cluster, then sort them so that they are processed in order. Lastly, we fetch the document from that index.\n\ndef get_key_chunks(vectors, alg, num_clusters, documents):\n    closest_indices = []\n\n    for i in range(num_clusters):\n        distances = np.linalg.norm(vectors - alg.cluster_centers_[i], axis=1)\n\n        closest_index = np.argmin(distances)\n        closest_indices.append(closest_index)\n\n    selected_indices = sorted(closest_indices)\n    selected_docs = [documents[doc] for doc in selected_indices]\n    return selected_docs\n\n\nfrom langchain import PromptTemplate\n\nmap_prompt = \"\"\"\nYou will be given a piece of a larger text. This piece of text will be enclosed in triple backticks (```).\nYour job is to give a summary of this piece of text so that the reader will have a full understanding of what the text is about.\nYour response should be at least three paragraphs and fully encompass what was written in the piece of text.\n\n```{text}```\nFULL SUMMARY:\n\"\"\"\n\nmap_prompt_template = PromptTemplate(template=map_prompt, input_variables=[\"text\"])\n\n\nfrom langchain.chains.summarize import load_summarize_chain\n\nmap_chain = load_summarize_chain(llm=llm, chain_type=\"stuff\", prompt=map_prompt_template)\n\nsummary_list = []\n\nselected_docs = get_key_chunks(vectors, kmeans, num_clusters, split_docs)\n\nfor i, doc in enumerate(selected_docs):\n    chunk_summary = map_chain.run([doc])\n    summary_list.append(chunk_summary)\n\n    print(f'Summary #{i} - Preview: {chunk_summary[:250]} \\n')\n\nSummary #0 - Preview: The given text provides recommendations on choosing a Content Management System (CMS) for Nord University. It highlights four platforms that stand out: Wordpress, Episerver, Umbraco, and Craft. Episerver and Umbraco, being part of the Microsoft famil \n\nSummary #1 - Preview: The text outlines the terms and conditions regarding the pricing, extension, and responsibilities of the service provider (Leverandøren) and the customer (Kunden) in a contractual agreement. It specifies that the customer is not required to pay any f \n\n\n\n\nllm4 = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4-turbo-preview\")\n\n\nfrom langchain.schema import Document\nsummaries = \"\\n\".join(summary_list)\n\nsummaries = Document(page_content=summaries)\n\nprint(f'All your summaries together are {llm.get_num_tokens(summaries.page_content)} tokens')\n\nAll your summaries together are 540 tokens\n\n\n\ncombine_prompt = \"\"\"\nYou will now be given a series of summaries from a larger text. The summaries will be enclosed in triple backticks(```).\nYour goal is to give a summary of what happened in the greater piece of text.\nThe reader should be able to grasp what the full text is about from your summary.\n\n```{text}```\nSUMMARY:\n\"\"\"\n\ncombine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=[\"text\"])\n\n\nreduce_chain = load_summarize_chain(llm=llm4, chain_type=\"stuff\", prompt=combine_prompt_template)\n\noutput = reduce_chain.run([summaries])\n\n\nprint(output)\n\nThe larger text provides a comprehensive guide on selecting a Content Management System (CMS) for Nord University, evaluating platforms like Wordpress, Episerver, Umbraco, and Craft based on various criteria including cost, SEO, user experience, and their current phases of development. It emphasizes the need for the university's website to quickly convert visitors into users by being visually appealing, user-friendly, and effectively targeting various user groups to relevant content and landing pages. The text also covers technical aspects of CMS setup such as template design, validation, SEO, user experience checks, and the importance of a streamlined website architecture through access control, data management, and content hierarchy. Additionally, it discusses SEO strategies, sustainability practices, and performance metrics vital for the website's success.\n\nMoreover, the text outlines contractual terms between the service provider and the customer, detailing conditions around fees, termination rights, temporary extensions, and responsibilities related to service delivery, personnel competence, and subcontractor management. It specifies the customer's rights concerning withholding payments for unmet service standards, particularly in relation to labor laws and working conditions, and outlines the obligations of both parties within the service agreement. Overall, the text serves as a detailed guide for Nord University in revamping its website, focusing on both the strategic selection of a CMS platform and the legal and operational considerations of engaging with a service provider."
  },
  {
    "objectID": "posts/presentation/presentation.html",
    "href": "posts/presentation/presentation.html",
    "title": "Presentation",
    "section": "",
    "text": "This is a post with executable code.\n%pip install langchain langchain-community langchain-openai unstructured openai pypdf -Uq\n\n\n[notice] A new release of pip is available: 23.2.1 -&gt; 24.0\n[notice] To update, run: pip3 install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\nfrom langchain.document_loaders import PyPDFLoader, UnstructuredExcelLoader, UnstructuredWordDocumentLoader\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nPERSONAL_API_KEY = os.getenv(\"MY_OPEN_AI_API_KEY\")\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(openai_api_key=PERSONAL_API_KEY, model=\"gpt-4-0125-preview\")"
  },
  {
    "objectID": "posts/presentation/presentation.html#load-test-documents",
    "href": "posts/presentation/presentation.html#load-test-documents",
    "title": "Presentation",
    "section": "Load test documents",
    "text": "Load test documents\nDocument loaders\n\nloader_word = UnstructuredWordDocumentLoader(\"../../content/DPS Kvalifikasjonsgrunnlag - applikasjonsforvaltning V 2.0.docx\")\nloader_pdf = PyPDFLoader(\"../../content/Del-A-Konkurransegrunnlag-aapen-anbudskonkurranse-FOA-del-III xx.pdf\")\nloader_excel = UnstructuredExcelLoader(\"../../content/test.xlsx\")\n\nLoad data from files\n\ndata_word = loader_word.load()\ndata_pdf = loader_pdf.load()\ndata_excel = loader_excel.load()\n\n\nimport re\n\ndef clean_text(text: str):\n    # Remove excessive newlines and keep only ASCII + æøå characters.\n    text = re.sub(r'\\n{2,}', '\\n', text)\n    text = re.sub(r'[^\\x00-\\x7FæøåÆØÅ]+', '', text)\n    # Remove empty strings\n    text = \"\\n\".join([line for line in text.split('\\n') if line.strip() != ''])\n    return text\n\n\nfrom langchain_core.documents import Document\nimport pypdf\n\ndef process_pdf(data) -&gt; Document:\n    \"\"\"\n    Reads a pdf file from the stream, and returns the read text as a Document\n    \"\"\"\n    reader = pypdf.PdfReader(data)\n    text = ''\n    for page_num in range(len(reader.pages)):\n        text += reader.pages[page_num].extract_text()\n    cleaned_text = clean_text(text)\n    doc = Document(page_content=cleaned_text)\n    return doc\n\n\ndoc = process_pdf(\"../../content/Del-A-Konkurransegrunnlag-aapen-anbudskonkurranse-FOA-del-III xx.pdf\")\n\n\nimport csv\ntest_file_name = '../../content/test_file.csv'\nwith open(test_file_name, 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow(['Column1', 'Column2', 'Column3'])\n    writer.writerow(['Data1', 'Data2', 'Data3'])\n\n\nfrom langchain_community.document_loaders import UnstructuredCSVLoader\nloader_csv = UnstructuredCSVLoader(\"../../content/test_file.csv\")\ndata_csv = loader_csv.load()\n\n\ndata_csv\n\n[Document(page_content='\\n\\n\\nColumn1\\nColumn2\\nColumn3\\n\\n\\nData1\\nData2\\nData3\\n\\n\\n', metadata={'source': '../../content/test_file.csv'})]"
  },
  {
    "objectID": "posts/map-reduction/map-reduction.html",
    "href": "posts/map-reduction/map-reduction.html",
    "title": "Map Reduction",
    "section": "",
    "text": "A good way to work around token limitations. If you are summarizing text that is too long for your chosen model’s context window, Map Reduction can be used.\nMap Reduction works like this: - The text is broken up into manageable pieces (chunks) - A summary is generated for each chunk - A final summary is generated from all the chunk summaries\nThis method is well suited for generating summaries of some types of text, but has some limitations: - The model may over or underemphazise certain aspects of the text - Gets really expensive really fast, as you need many calls to the LLM and lots of input tokens.\n\nfrom dotenv import load_dotenv\nfrom utils import read_files, split_document_by_tokens\nfrom pathlib import Path\n\nimport os\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\ndocuments = read_files(Path('../../content/books'))\n\n\nsummary_map_template = \"\"\"Write a short summary of the following text:\n\n{context}\n\nSUMMARY:\n\"\"\"\n\nsummary_reduce_template = \"\"\"The following text is a set of summaries:\n\n{doc_summaries}\n\nCreate a cohesive summary from the above text.\nSUMMARY:\"\"\"\n\n\nfrom langchain.chains import LLMChain, ReduceDocumentsChain, MapReduceDocumentsChain, StuffDocumentsChain\nfrom langchain.docstore.document import Document\nfrom langchain.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI\n\ndef summarize_document(document: list[Document]):\n    # Chain to generate a summary from each chunk\n    llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4-0125-preview\")\n    map_prompt = PromptTemplate.from_template(summary_map_template)\n    map_chain = LLMChain(prompt=map_prompt, llm=llm)\n\n    # Chain to generate one cohesive summary from the summaries\n    reduce_prompt = PromptTemplate.from_template(summary_reduce_template)\n    reduce_chain = LLMChain(prompt=reduce_prompt, llm=llm)\n    stuff_chain = StuffDocumentsChain(llm_chain=reduce_chain, document_variable_name=\"doc_summaries\")\n    reduce_docs_chain = ReduceDocumentsChain(combine_documents_chain=stuff_chain)\n\n    # The complete map reduction chain\n    map_reduce_chain = MapReduceDocumentsChain(\n        llm_chain=map_chain,\n        document_variable_name=\"content\",\n        reduce_documents_chain=reduce_docs_chain\n    )\n\n    splitdocs = split_document_by_tokens(document, 15000, 200)\n    summary = map_reduce_chain.run(splitdocs)\n    return summary"
  },
  {
    "objectID": "posts/summarization/index.html",
    "href": "posts/summarization/index.html",
    "title": "Summarization techniques using Large Language Models",
    "section": "",
    "text": "This blog post talks about our course project in DAT255 Deep Learning Engineering.\nThe course project is related to our bachelor thesis. As part of our project, we are to create summaries for tender competitions (Anbudskonkurranser). These competitions come with a varying amount of documents, with varying length, file format and quality. So, this course project will be preparatory work for our bachelor project. We wish to explore different methods to summarize text using commercially available LLMs (Large Language Models). Since we do not know the specific documents we are to summarize, no specific cleaning or preprocessing can be done, everything has to be generic."
  },
  {
    "objectID": "posts/summarization/index.html#document-summaries-with-langchain",
    "href": "posts/summarization/index.html#document-summaries-with-langchain",
    "title": "Summarization techniques using Large Language Models",
    "section": "Document Summaries with Langchain",
    "text": "Document Summaries with Langchain\n  In this project we have been exploring different methods to summarize text using commercially available LLMs. To do this, we have used the framework Langchain extensively.\nLangchain is a framework for developing applications powered by language models. You can read more about Langchain here.\nThe summarization problem\n  All LLMs have a token limit due to how they are designed and trained. This token limit gives a restriction on how much input you can give the LLM, and how much output you can expect back.\nHere are some well known LLMs and their token limits:\n\n\n\nModel\nToken Limit\nMax Output Tokens\n\n\n\n\nGPT-4-Turbo-Preview\n128 000\n4096\n\n\nGPT-3.5-Turbo\n16 385\n4096\n\n\nClaude 3 Opus/Sonnet/Haiku\n200 000\n4096\n\n\n\nAs we can see, most modern LLMs have a pretty big context window. But what if you want to summarize a huge document with over 500 pages, or maybe you have several documents to combine and summarize? This can quickly exceed the token limit if you want to do this in a single call to the LLM.\nThis means that summarizing the entire document at once is not always feasible, so we need new strategies for generating good summaries for long texts.\nAnd, even if summarizing the entire document is possible, cost is an important factor. Commercial LLMs charge money per input and output token, and summarizing many huge documents will quickly rack up your bill.\n\nStuffing the documents\nDocument stuffing is a method used for smaller documents. Like the name says, this method “Stuffs” the document or documents into the prompt. In Langchain a Chain called “StuffDocumentsChain” is used.\nA StuffDocumentsChain prompt will typically describe the task, and then insert the document(s). However, as we have discussed earlier, this is not a good approch if you have large documents, because this chain only queries the API with one API call containing the whole document.\n\n\n\nMap Reduction\nMap reduction is another common approach for summarizing documents. This method is able to summarize documents which exceed the LLMs token limit by first breaking the documents into chunks which fit in the context window, then generating summaries for each chunk, and lastly generating a final summary from all the summaries.\nThis method lets us generate summaries for texts of arbitrary length (if the combined summaries are still too long, you can generate a summary from a group of summaries until they will fit in the context window), but as discussed earlier, cost is a problem. Map Reduction does many calls to the API, and will use a lot of input and output tokens in the process."
  },
  {
    "objectID": "posts/summarization/index.html#clustering",
    "href": "posts/summarization/index.html#clustering",
    "title": "Summarization techniques using Large Language Models",
    "section": "Clustering",
    "text": "Clustering\nIn this method, you first break the document into chunks, then generate embeddings from these chunks. An embedding in this context is a vector representation of text. We use embeddings because machine learning models work with numbers only, and cannot understand human readable text directly. Embeddings contain many dimensions and captures the semantic and syntactic meaning of a piece of text. If you embed many different words, semantically similar words like tree and forest will end up closer together in the vector space than semantically different words like lion and truck.\nWe can use this to our advantage when summarizing documents. If we embed all our chunks, then chunks that are talking about the same topic will be closer together in the vector space. Then, we can cluster the chunks together based on their semantic meaning. The image below shows a “squashed down” visualization (the embedding vectors have a dimension of 1536, here they are reduced to 2 dimensions) of the embedding vector space for a tender competition.\n\nTo create these clusters, a clustering algorithm is used. The algorithm identifies clusters, then we find the center of the cluster and extract the nearest chunk, which will represent the “average meaning” of that cluster.\nThe goal of this method is to identify key topics in the text and assemble them to create a context-rich summary while spending as little as possible on API fees.\n\nSources / Further reading\nSummarizing long documents with AI - https://pashpashpash.substack.com/p/tackling-the-challenge-of-document\n5 Levels of Summarization: Novice to Expert - https://github.com/gkamradt/langchain-tutorials/blob/main/data_generation/5%20Levels%20Of%20Summarization%20-%20Novice%20To%20Expert.ipynb\nClustering for QA - https://github.com/mendableai/QA_clustering/blob/main/notebooks/clustering_approach.ipynb"
  },
  {
    "objectID": "posts/kmeans-clustering/clustering.html",
    "href": "posts/kmeans-clustering/clustering.html",
    "title": "KMeans Clustering",
    "section": "",
    "text": "When you reach a certain length of text to summarize, the other methods become too expensive or are not able to summarize well enough. We need to find a way to extract all the important parts of large texts like books or very big documents, and create a summary from them.\nSome simple ways to avoid using all the chunks for summarization is either randomly selecting chunks, or spacing out which chunks are selected. But what if we miss out on an important part of the text while doing this?\nA solution is K-means clustering, where each chunk is embedded, and then clusters are formed based on semantic meaning of those chunks. Then a summary is formed from each cluster, hopefully netting us a more accurate summary of huge texts.\nAnother advantage is the amount of requests sent to the API. While Map Reduction sends many requests, clustering will send only one request, saving a lot of money.\n\nImplementing KMeans Clustering\nLet’s implement this method to summarize many documents or whole books!\nFirst, we need to import some packages and load in our OpenAI API key, since we will be using OpenAI’s GPT models.\n\n\nCode\nfrom dotenv import load_dotenv\nfrom utils import read_files, split_document_by_tokens\nfrom pathlib import Path\nimport os\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n\n\nNext, we will load in our documents. Let’s load in a whole tender competition.\n\n\nCode\ndocuments = read_files(Path('./content/nord-universitet'))\ndoc = documents[1]\nprint(document.metadata for document in documents)\n\n\n&lt;generator object &lt;genexpr&gt; at 0x3101d5ff0&gt;"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploration of summarization techniques using machine learning",
    "section": "",
    "text": "Clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummarization techniques using Large Language Models\n\n\n\n\n\n\nllm\n\n\n\n\n\n\n\n\n\nApr 16, 2024\n\n\nNora Kristiansen og Torbjørn Vatnelid\n\n\n\n\n\n\n\n\n\n\n\n\nPresentation\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nllm\n\n\n\n\n\n\n\n\n\nApr 15, 2024\n\n\nNora Kristiansen, Torjørn Vatnelid\n\n\n\n\n\n\n\n\n\n\n\n\nKMeans Clustering\n\n\n\n\n\n\nllm\n\n\n\n\n\n\n\n\n\nApr 15, 2024\n\n\nNora Kristiansen og Torbjørn Vatnelid\n\n\n\n\n\n\n\n\n\n\n\n\nMap Reduction\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nllm\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\nNora Kristiansen, Torjørn Vatnelid\n\n\n\n\n\n\nNo matching items"
  }
]