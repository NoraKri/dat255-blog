{
  "hash": "01ca3c28968dcd2b682017a36816947c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"KMeans Clustering\"\nauthor: \"Nora Kristiansen og Torbj√∏rn Vatnelid\"\ndate: \"2024-04-15\"\ncategories: [llm]\n---\n\nWhen you reach a certain length of text to summarize, the other methods become too expensive or are not able to summarize well enough. We need to find a way to extract all the important parts of large texts like books or very big documents, and create a summary from them.\n\nSome simple ways to avoid using all the chunks for summarization is either randomly selecting chunks, or spacing out which chunks are selected. But what if we miss out on an important part of the text while doing this?\n\nA solution is K-means clustering, where each chunk is embedded, and then clusters are formed based on semantic meaning of those chunks. Then a summary is formed from each cluster, hopefully netting us a more accurate summary of huge texts.\n\nAnother advantage is the amount of requests sent to the API. While Map Reduction sends many requests, clustering will send only one request, saving a lot of money.\npy\n\n",
    "supporting": [
      "clustering_files"
    ],
    "filters": [],
    "includes": {}
  }
}