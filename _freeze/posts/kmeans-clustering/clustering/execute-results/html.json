{
  "hash": "357746abe573189dac5e2bb97cc23534",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"KMeans Clustering\"\nauthor: \"Nora Kristiansen og Torbj√∏rn Vatnelid\"\ndate: \"2024-04-15\"\ncategories: [llm]\n---\n\nWhen you reach a certain length of text to summarize, the other methods become too expensive or are not able to summarize well enough. We need to find a way to extract all the important parts of large texts like books or very big documents, and create a summary from them.\n\nSome simple ways to avoid using all the chunks for summarization is either randomly selecting chunks, or spacing out which chunks are selected. But what if we miss out on an important part of the text while doing this?\n\nA solution is K-means clustering, where each chunk is embedded, and then clusters are formed based on semantic meaning of those chunks. Then a summary is formed from each cluster, hopefully netting us a more accurate summary of huge texts.\n\nAnother advantage is the amount of requests sent to the API. While Map Reduction sends many requests, clustering will send only one request, saving a lot of money.\n\n### Implementing KMeans Clustering\nLet's implement this method to summarize many documents or whole books!\n\nFirst, we need to import some packages and load in our OpenAI API key, since we will be using OpenAI's GPT models.\n\n::: {#d2b37ee7 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nfrom dotenv import load_dotenv\nfrom utils import read_files, split_document_by_tokens\nfrom pathlib import Path\nimport os\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n```\n:::\n\n\nNext, we will load in our documents. Let's load in a whole tender competition.\n\n::: {#1a466118 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\ndocuments = read_files(Path('./content/nord-universitet'))\ndoc = documents[1]\nprint(document.metadata for document in documents)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<generator object <genexpr> at 0x3101d5ff0>\n```\n:::\n:::\n\n\n",
    "supporting": [
      "clustering_files"
    ],
    "filters": [],
    "includes": {}
  }
}