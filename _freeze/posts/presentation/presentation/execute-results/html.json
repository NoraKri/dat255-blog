{
  "hash": "03e4ba1d9cea9c8663cbddecc9c2a1dd",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Presentation\"\nauthor: \"Nora Kristiansen, Torjørn Vatnelid\"\ndate: \"2024-04-15\"\ncategories: [code, analysis, llm]\n---\n\nThis is a post with executable code.\n\n::: {#631258a9 .cell executionInfo='{\"elapsed\":3893,\"status\":\"ok\",\"timestamp\":1711013531109,\"user\":{\"displayName\":\"Nora Kristiansen\",\"userId\":4695517471848529000},\"user_tz\":-60}' execution_count=1}\n``` {.python .cell-code}\n%pip install langchain langchain-community langchain-openai unstructured openai pypdf -Uq\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r\n[notice] A new release of pip is available: 23.2.1 -> 24.0\r\n[notice] To update, run: pip3 install --upgrade pip\r\nNote: you may need to restart the kernel to use updated packages.\n```\n:::\n:::\n\n\n::: {#2ebebf2a .cell executionInfo='{\"elapsed\":1306,\"status\":\"ok\",\"timestamp\":1711013435174,\"user\":{\"displayName\":\"Nora Kristiansen\",\"userId\":4695517471848529000},\"user_tz\":-60}' execution_count=2}\n``` {.python .cell-code}\nfrom langchain.document_loaders import PyPDFLoader, UnstructuredExcelLoader, UnstructuredWordDocumentLoader\n```\n:::\n\n\n::: {#ae35b874 .cell execution_count=3}\n``` {.python .cell-code}\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nPERSONAL_API_KEY = os.getenv(\"MY_OPEN_AI_API_KEY\")\n```\n:::\n\n\n::: {#8a393f05 .cell executionInfo='{\"elapsed\":2397,\"status\":\"ok\",\"timestamp\":1711013437568,\"user\":{\"displayName\":\"Nora Kristiansen\",\"userId\":4695517471848529000},\"user_tz\":-60}' execution_count=4}\n``` {.python .cell-code}\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(openai_api_key=PERSONAL_API_KEY, model=\"gpt-4-0125-preview\")\n```\n:::\n\n\n## Load test documents\n\nDocument loaders\n\n::: {#740588b6 .cell executionInfo='{\"elapsed\":524,\"status\":\"ok\",\"timestamp\":1711013438089,\"user\":{\"displayName\":\"Nora Kristiansen\",\"userId\":4695517471848529000},\"user_tz\":-60}' execution_count=5}\n``` {.python .cell-code}\nloader_word = UnstructuredWordDocumentLoader(\"../../content/DPS Kvalifikasjonsgrunnlag - applikasjonsforvaltning V 2.0.docx\")\nloader_pdf = PyPDFLoader(\"../../content/Del-A-Konkurransegrunnlag-aapen-anbudskonkurranse-FOA-del-III xx.pdf\")\nloader_excel = UnstructuredExcelLoader(\"../../content/test.xlsx\")\n```\n:::\n\n\nLoad data from files\n\n::: {#ecd7e1b0 .cell executionInfo='{\"elapsed\":5,\"status\":\"ok\",\"timestamp\":1711013438089,\"user\":{\"displayName\":\"Nora Kristiansen\",\"userId\":4695517471848529000},\"user_tz\":-60}' execution_count=6}\n``` {.python .cell-code}\ndata_word = loader_word.load()\ndata_pdf = loader_pdf.load()\ndata_excel = loader_excel.load()\n```\n:::\n\n\n::: {#71ed5054 .cell execution_count=7}\n``` {.python .cell-code}\nimport re\n\ndef clean_text(text: str):\n    # Remove excessive newlines and keep only ASCII + æøå characters.\n    text = re.sub(r'\\n{2,}', '\\n', text)\n    text = re.sub(r'[^\\x00-\\x7FæøåÆØÅ]+', '', text)\n    # Remove empty strings\n    text = \"\\n\".join([line for line in text.split('\\n') if line.strip() != ''])\n    return text\n```\n:::\n\n\n::: {#35f5c9f7 .cell executionInfo='{\"elapsed\":3,\"status\":\"ok\",\"timestamp\":1711013566686,\"user\":{\"displayName\":\"Nora Kristiansen\",\"userId\":4695517471848529000},\"user_tz\":-60}' execution_count=8}\n``` {.python .cell-code}\nfrom langchain_core.documents import Document\nimport pypdf\n\ndef process_pdf(data) -> Document:\n    \"\"\"\n    Reads a pdf file from the stream, and returns the read text as a Document\n    \"\"\"\n    reader = pypdf.PdfReader(data)\n    text = ''\n    for page_num in range(len(reader.pages)):\n        text += reader.pages[page_num].extract_text()\n    cleaned_text = clean_text(text)\n    doc = Document(page_content=cleaned_text)\n    return doc\n```\n:::\n\n\n::: {#136b8aa0 .cell executionInfo='{\"elapsed\":6,\"status\":\"error\",\"timestamp\":1711013706986,\"user\":{\"displayName\":\"Nora Kristiansen\",\"userId\":4695517471848529000},\"user_tz\":-60}' execution_count=9}\n``` {.python .cell-code}\ndoc = process_pdf(\"../../content/Del-A-Konkurransegrunnlag-aapen-anbudskonkurranse-FOA-del-III xx.pdf\")\n```\n:::\n\n\n::: {#650dff50 .cell executionInfo='{\"elapsed\":4,\"status\":\"ok\",\"timestamp\":1710927452450,\"user\":{\"displayName\":\"Nora Kristiansen\",\"userId\":4695517471848529000},\"user_tz\":-60}' execution_count=10}\n``` {.python .cell-code}\nimport csv\ntest_file_name = '../../content/test_file.csv'\nwith open(test_file_name, 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow(['Column1', 'Column2', 'Column3'])\n    writer.writerow(['Data1', 'Data2', 'Data3'])\n```\n:::\n\n\n::: {#6642bf77 .cell executionInfo='{\"elapsed\":2782,\"status\":\"ok\",\"timestamp\":1710928152148,\"user\":{\"displayName\":\"Nora Kristiansen\",\"userId\":4695517471848529000},\"user_tz\":-60}' execution_count=11}\n``` {.python .cell-code}\nfrom langchain_community.document_loaders import UnstructuredCSVLoader\nloader_csv = UnstructuredCSVLoader(\"../../content/test_file.csv\")\ndata_csv = loader_csv.load()\n```\n:::\n\n\n::: {#e3eb70bb .cell executionInfo='{\"elapsed\":302,\"status\":\"ok\",\"timestamp\":1710928160634,\"user\":{\"displayName\":\"Nora Kristiansen\",\"userId\":4695517471848529000},\"user_tz\":-60}' execution_count=12}\n``` {.python .cell-code}\ndata_csv\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n[Document(page_content='\\n\\n\\nColumn1\\nColumn2\\nColumn3\\n\\n\\nData1\\nData2\\nData3\\n\\n\\n', metadata={'source': '../../content/test_file.csv'})]\n```\n:::\n:::\n\n\n# Chunk documents\nWe split the documents by tokens\n\n::: {#7b0fd636 .cell executionInfo='{\"elapsed\":301,\"status\":\"ok\",\"timestamp\":1710840992373,\"user\":{\"displayName\":\"Nora Kristiansen\",\"userId\":4695517471848529000},\"user_tz\":-60}' execution_count=13}\n``` {.python .cell-code}\nfrom langchain_core.documents import Document\nfrom langchain.text_splitter import TokenTextSplitter\n\n# Take in a document and chunk it if neccessary. Splits on token length.\ndef split_document_by_tokens(document: list[Document], chunk_size: int, overlap: int):\n    splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n    return splitter.split_documents(document)\n```\n:::\n\n\n# Summarize Document(s)\n\n**Dette funker ikke lengre, null peiling hvorfor**\n\nOppsummer ett eller flere dokumenter ved hjelp av map reduction.\n\n1: Map hver chunk til en oppsummering av chunken\n\n2: Reduser alle oppsummeringer til én enkelt oppsummering\n\n::: {#900d833d .cell execution_count=14}\n``` {.python .cell-code}\nsummary_map_template = \"\"\"Skriv en kortfattet oppsummering av følgende innhold:\n\n{content}\n\nOPPSUMMERING:\n\"\"\"\n\nsummary_reduce_template = \"\"\"Følgende er et sett med oppsummeringer:\n\n{doc_summaries}\n\nLag en sammenhengende oppsumering ut fra disse.\nOPPSUMMERING:\"\"\"\n```\n:::\n\n\n::: {#985aa8a6 .cell execution_count=15}\n``` {.python .cell-code}\nfrom langchain.chains import LLMChain, ReduceDocumentsChain, MapReduceDocumentsChain, StuffDocumentsChain\nfrom langchain.prompts import PromptTemplate\n\ndef summarize_document(document: list[Document]):\n    \"\"\"\n    Takes in a list of Documents and summarizes them.\n    :param document: The document(s) to be summarized.\n    :return: A dict of named outputs Dict[str, Any].\n    \"\"\"\n    # Chain to generate a summary from each chunk\n    map_prompt = PromptTemplate.from_template(summary_map_template)\n    map_chain = LLMChain(prompt=map_prompt, llm=llm)\n\n    # Chain to generate one cohesive summary from the summaries\n    reduce_prompt = PromptTemplate.from_template(summary_reduce_template)\n    reduce_chain = LLMChain(prompt=reduce_prompt, llm=llm)\n    stuff_chain = StuffDocumentsChain(llm_chain=reduce_chain, document_variable_name=\"doc_summaries\")\n    reduce_docs_chain = ReduceDocumentsChain(combine_documents_chain=stuff_chain)\n\n    # The complete map reduction chain\n    map_reduce_chain = MapReduceDocumentsChain(\n        llm_chain=map_chain,\n        document_variable_name=\"content\",\n        reduce_documents_chain=reduce_docs_chain\n    )\n\n    splitdocs = split_document_by_tokens(document, 15000, 200)\n    summary = map_reduce_chain.run(splitdocs)\n    return summary\n```\n:::\n\n\n::: {#b037fd4f .cell executionInfo='{\"elapsed\":253,\"status\":\"error\",\"timestamp\":1710160911975,\"user\":{\"displayName\":\"Nora Kristiansen\",\"userId\":4695517471848529000},\"user_tz\":-60}' execution_count=16}\n``` {.python .cell-code}\nsummarized_word = summarize_document(data_pdf)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n  warn_deprecated(\n```\n:::\n:::\n\n\n::: {#c449f527 .cell execution_count=17}\n``` {.python .cell-code}\nsummarized_word\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n'Statens pensjonskasse (SPK) har utlyst en åpen anbudskonkurranse for innkjøp av programvareløsninger, med mål om å inngå en rammeavtale for levering, rådgivning og vedlikehold av programvarelisenser. Konkurransen, som har et anslått kontraktsomfang på 80 til 150 millioner kroner ekskl. mva. over en periode på 2 år med mulighet for forlengelse opptil 6 år, er designet for å være transparent og rettferdig, oppfyller de nødvendige kravene i Forskrift om offentlige anskaffelser (FOA) del I og III, og overskrider EØS-terskelen. \\n\\nAnbudsprosessen vektlegger miljøvennlige løsninger og setter krav til leverandørenes kvalifikasjoner, økonomiske og finansielle kapasitet, samt tekniske og faglige kvalifikasjoner. Det kreves bruk av det europeiske egenerklæringsskjemaet (ESPD) for foreløpig bekreftelse på kvalifikasjoner, og leverandører må levere detaljert dokumentasjon, inkludert et signert tilbudsbrev, besvarelse på kravspesifikasjonen, og priser. Tilbudsprosedyren understreker også behovet for å beskytte konfidensiell informasjon, mens det oppfordres til å inkludere en sladdet versjon av tilbudet for innsynsforespørsler.\\n\\nTildelingskriteriene inneholder en balanse mellom kvalitative vurderinger og pris-/kostnadsevalueringer, med særlig fokus på nye dataplattformlisenser, kvalitet, tjenestenivå, support, og klima- og miljøhensyn. Kommunikasjon rundt konkurransen og innsendelse av tilbud skal foregå gjennom Mercell-plattformen, med en frist for tilbudslevering satt til 2. mai 2024.\\n\\nSPK forbeholder seg retten til å avlyse konkurransen under visse omstendigheter, med skriftlig meddelelse til alle deltakere. Dette initiativet reflekterer SPKs engasjement for rettferdige, effektive og bærekraftige anskaffelsesprosesser, samt deres overordnede mål om å fremme bærekraft i sine operasjoner.'\n```\n:::\n:::\n\n\n",
    "supporting": [
      "presentation_files"
    ],
    "filters": [],
    "includes": {}
  }
}