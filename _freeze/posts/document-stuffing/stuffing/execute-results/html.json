{
  "hash": "d84b9c53c38927d25fb4012547c152bf",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Document Stuffing\nauthor: 'Nora Kristiansen, Torbj√∏rn Vatnelid'\ndate: '2024-04-15'\nformat:\n  html:\n    code-overflow: wrap\n---\n\nThe simplest form of summarization, where you simply \"stuff\" the document into your prompt. Works well for small amounts of text that fit into the model's context window.\n\nSome newer models have huge context windows of 200k+ tokens, but one should still be careful about stuffing huge documents like books straight into the model for summarization. The reason for this is that commercial models charge for the tokens used, so stuffing will become expensive as the text length grows. In addition, most models have an output token limit of a bit over 4 thousand tokens, limiting how much text we can get out.\n\nTo get us started, let's load in some documents and fetch our OpenAI API key.\n\n::: {#d628f9b6 .cell execution_count=1}\n``` {.python .cell-code}\nfrom dotenv import load_dotenv\nfrom utils import read_files\nfrom pathlib import Path\n\nimport os\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\ndocuments = read_files(Path('./content/nord-universitet'))\n```\n:::\n\n\nWe need a prompt for summarizing our stuffed document. Our prompt will have a variable, context, which contains our document.\n\n::: {#e842dcff .cell execution_count=2}\n```` {.python .cell-code}\nstuff_prompt = \"\"\"Write a concise summary of the following text enclosed in triple backticks (```).\nIt is important that your response is in the same language as the text:\n\n```{context}```\n\nSUMMARY:\n\"\"\"\n````\n:::\n\n\nLangChain's Prompt Templates make it super easy to insert variables into our prompts. We simply write out our prompt, with a variable where we would like our document to go, and then define a PromptTemplate which specifies that variable as an input variable to our prompt.\n\nLet's try stuffing a whole tender competition with 15 files directly into OpenAI's GPT 3.5 Turbo model! \n\n::: {#e2d009eb .cell execution_count=3}\n``` {.python .cell-code}\nfrom langchain.chains.combine_documents.stuff import create_stuff_documents_chain\nfrom langchain.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom openai import RateLimitError\nimport textwrap\n\nstuff_prompt_template = PromptTemplate(\n    template=stuff_prompt,\n    input_variables=[\"context\"]\n)\n\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", api_key=OPENAI_API_KEY)\nchain = create_stuff_documents_chain(llm=llm, prompt=stuff_prompt_template)\n\ntry:\n    result = chain.invoke({\"context\": documents})\nexcept RateLimitError as e:\n    print(e)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError code: 429 - {'error': {'message': 'Request too large for gpt-3.5-turbo in organization org-QszCmPHnBJ7wwGx6aJTDzZmp on tokens per min (TPM): Limit 60000, Requested 107761. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n```\n:::\n:::\n\n\nWhoops, that didn't go so well. Seems we exceeded the token limit of our model. We'll explore how to remedy this in later posts, but for now, let's just stuff the first document instead.\n\n::: {#245c6abf .cell execution_count=4}\n``` {.python .cell-code}\ndocs = [documents[0]]\ntextwrap.wrap(text=chain.invoke({\"context\": docs}), drop_whitespace=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n['This text is a competition announcement for the procurement of new ',\n 'websites and a publishing solution (CMS) for Nord University. The ',\n 'purpose is to engage a supplier to assist with the development, ',\n 'design, and implementation of a new publishing solution for the ',\n 'external website www.nord.no, including maintenance and operation. The',\n ' estimated value of the procurement is between 3 and 4 MNOK excluding ',\n 'VAT. The competition will be limited to between five and seven ',\n 'suppliers. The text outlines the requirements, procedures, and ',\n 'evaluation criteria for the competition. The communication and ',\n 'documentation related to the competition must be in Norwegian. The ',\n 'text also provides information on deadlines, contract terms, ',\n 'confidentiality, and the evaluation process.']\n```\n:::\n:::\n\n\nAs we can see, stuffing works perfectly well for summarizing text that fits in the model's context window.\n\n",
    "supporting": [
      "stuffing_files"
    ],
    "filters": [],
    "includes": {}
  }
}