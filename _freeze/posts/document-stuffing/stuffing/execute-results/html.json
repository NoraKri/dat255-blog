{
  "hash": "06aaf87caf606421e400bfd67bd096b2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Document Stuffing\nauthor: 'Nora Kristiansen, Torbjørn Vatnelid'\ndate: '2024-04-15'\nformat:\n  html:\n    code-overflow: wrap\n---\n\nThe simplest form of summarization, where you simply \"stuff\" the document into your prompt. Works well for small amounts of text that fit into the model's context window.\n\nSome newer models have huge context windows of 200k+ tokens, but one should still be careful about stuffing huge documents like books straight into the model for summarization. The reason for this is that commercial models charge for the tokens used, so stuffing will become expensive as the text length grows. In addition, most models have an output token limit of a bit over 4 thousand tokens, limiting how much text we can get out.\n\nTo get us started, let's load in some documents and fetch our OpenAI API key.\n\n::: {#490fb96d .cell execution_count=1}\n``` {.python .cell-code}\nfrom dotenv import load_dotenv\nfrom utils import read_files\nfrom pathlib import Path\n\nimport os\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\ndocuments = read_files(Path('./content/nord-universitet'))\n```\n:::\n\n\nWe need a prompt for summarizing our stuffed document. Our prompt will have a variable, context, which contains our document.\n\n::: {#da1fbcea .cell execution_count=2}\n```` {.python .cell-code}\nstuff_prompt = \"\"\"Write a concise summary of the following text enclosed in triple backticks (```).\nIt is important that your response is in the same language as the text:\n\n```{context}```\n\nSUMMARY:\n\"\"\"\n````\n:::\n\n\nLangChain's Prompt Templates make it super easy to insert variables into our prompts. We simply write out our prompt, with a variable where we would like our document to go, and then define a PromptTemplate which specifies that variable as an input variable to our prompt.\n\nLet's try stuffing a whole tender competition with 15 files directly into OpenAI's GPT 3.5 Turbo model! \n\n::: {#9bd7bcd3 .cell execution_count=3}\n``` {.python .cell-code}\nfrom langchain.chains.combine_documents.stuff import create_stuff_documents_chain\nfrom langchain.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom openai import RateLimitError\nimport textwrap\n\nstuff_prompt_template = PromptTemplate(\n    template=stuff_prompt,\n    input_variables=[\"context\"]\n)\n\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", api_key=OPENAI_API_KEY)\nchain = create_stuff_documents_chain(llm=llm, prompt=stuff_prompt_template)\n\ntry:\n    result = chain.invoke({\"context\": documents})\nexcept RateLimitError as e:\n    textwrap.wrap(e.message, drop_whitespace=False)\n```\n:::\n\n\nWhoops, that didn't go so well. Seems we exceeded the token limit of our model. We'll explore how to remedy this in later posts, but for now, let's just stuff the first document instead.\n\n::: {#fbe6c3ad .cell execution_count=4}\n``` {.python .cell-code}\ndocs = [documents[0]]\ntextwrap.wrap(text=chain.invoke({\"context\": docs}), drop_whitespace=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\n['Dette er et konkurransegrunnlag for anskaffelse av nye nettsider og ',\n 'publiseringsløsning for Nord universitet. Formålet er å få en stabil, ',\n 'brukervennlig plattform som støtter universitetets behov. Anskaffelsen',\n ' omfatter utvikling, implementering, opplæring, drift og vedlikehold ',\n 'av løsningen. Anslått verdi er 12 MNOK. Konkurransen gjennomføres iht.',\n ' offentlige anskaffelseslover, med begrensning i antall deltakere og ',\n 'utvelgelseskriterier basert på kvalifikasjoner. Konkurransegrunnlaget ',\n 'består av to deler med vedlegg som beskriver detaljer om oppdraget. ',\n 'Leverandører må oppfylle registreringskrav, økonomiske, tekniske og ',\n 'faglige kvalifikasjoner. Tildeling skjer basert på kvalitet, ',\n 'kompetanse, prosjektfremdrift, service og vedlikehold, og pris.```']\n```\n:::\n:::\n\n\n",
    "supporting": [
      "stuffing_files"
    ],
    "filters": [],
    "includes": {}
  }
}