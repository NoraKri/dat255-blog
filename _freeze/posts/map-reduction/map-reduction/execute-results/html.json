{
  "hash": "4ee641d56e17886caac7ebef193c0db2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Map Reduction\nauthor: 'Nora Kristiansen, Torj√∏rn Vatnelid'\ndate: '2024-02-15'\ncategories:\n  - code\n  - analysis\n  - llm\n---\n\nA good way to work around token limitations. If you are summarizing text that is too long for your chosen model's context window, Map Reduction can be used.\n\nMap Reduction works like this:\n- The text is broken up into manageable pieces (chunks)\n- A summary is generated for each chunk\n- A final summary is generated from all the chunk summaries\n\nThis method is well suited for generating summaries of some types of text, but has some limitations:\n- The model may over or underemphazise certain aspects of the text\n- Gets really expensive really fast, as you need many calls to the LLM and lots of input tokens.\n\n::: {#a5d59841 .cell execution_count=1}\n``` {.python .cell-code}\nfrom dotenv import load_dotenv\nfrom utils import read_files, split_document_by_tokens\nfrom pathlib import Path\n\nimport os\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\ndocuments = read_files(Path('../../content/books'))\n```\n:::\n\n\n::: {#c0246302 .cell execution_count=2}\n``` {.python .cell-code}\nsummary_map_template = \"\"\"Write a short summary of the following text:\n\n{context}\n\nSUMMARY:\n\"\"\"\n\nsummary_reduce_template = \"\"\"The following text is a set of summaries:\n\n{doc_summaries}\n\nCreate a cohesive summary from the above text.\nSUMMARY:\"\"\"\n```\n:::\n\n\n::: {#366abebd .cell execution_count=3}\n``` {.python .cell-code}\nfrom langchain.chains import LLMChain, ReduceDocumentsChain, MapReduceDocumentsChain, StuffDocumentsChain\nfrom langchain.docstore.document import Document\nfrom langchain.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI\n\ndef summarize_document(document: list[Document]):\n    # Chain to generate a summary from each chunk\n    llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4-0125-preview\")\n    map_prompt = PromptTemplate.from_template(summary_map_template)\n    map_chain = LLMChain(prompt=map_prompt, llm=llm)\n\n    # Chain to generate one cohesive summary from the summaries\n    reduce_prompt = PromptTemplate.from_template(summary_reduce_template)\n    reduce_chain = LLMChain(prompt=reduce_prompt, llm=llm)\n    stuff_chain = StuffDocumentsChain(llm_chain=reduce_chain, document_variable_name=\"doc_summaries\")\n    reduce_docs_chain = ReduceDocumentsChain(combine_documents_chain=stuff_chain)\n\n    # The complete map reduction chain\n    map_reduce_chain = MapReduceDocumentsChain(\n        llm_chain=map_chain,\n        document_variable_name=\"content\",\n        reduce_documents_chain=reduce_docs_chain\n    )\n\n    splitdocs = split_document_by_tokens(document, 15000, 200)\n    summary = map_reduce_chain.run(splitdocs)\n    return summary\n```\n:::\n\n\n",
    "supporting": [
      "map-reduction_files"
    ],
    "filters": [],
    "includes": {}
  }
}